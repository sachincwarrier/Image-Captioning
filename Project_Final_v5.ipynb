{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4QGIAaYfECI"
   },
   "source": [
    "# Show, Attend and Tell - Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgtUZzTDfECR"
   },
   "source": [
    " ## Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fv0IF4ZJ2e8J"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q36AlFpE4h9b"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_98A1NRvIE6"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "from torchvision import transforms\n",
    "from fastai.callbacks import lr_finder, SaveModelCallback, EarlyStoppingCallback,ReduceLROnPlateauCallback\n",
    "from torch.nn.utils.rnn import pack_padded_sequence \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from fastai.vision import learner, data\n",
    "from fastai.metrics import top_k_accuracy\n",
    "from fastai.text import *\n",
    "from fastai.vision import *\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.utils import shuffle\n",
    "from PIL import Image\n",
    "\n",
    "defaults.device = torch.device('cuda')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Axy9a5t4yHJS"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxzUFoOhfECR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IZKVMBvLoah"
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wotQT2-L4Mxq"
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uT1_KFQJvcaQ"
   },
   "outputs": [],
   "source": [
    "config_baseline = {\n",
    "    ## BEGIN FastText Config - DO NOT CHANGE ##\n",
    "    'embedding.name':'fasttext-word-freq-2-plus',\n",
    "    'embedding.folder': 'fasttext-word-freq-2-plus',\n",
    "    'append_sos_eos_tokens': True,\n",
    "    'embedding.dim': 300,\n",
    "    'start_token': '<start>',\n",
    "    'end_token': '<end>',\n",
    "    'unknown_token': '<unk>',\n",
    "    'pad_token': '<pad>',\n",
    "    ## END FastText Config ##\n",
    "\n",
    "    'cnn': 'vgg16', # {resnet101,vgg16,resnext101}\n",
    "    'encoder_dim': 512,\n",
    "    'encoder': 'lstm', #{lstm|transformer}\n",
    "    'search' : 'greedy',#{greedy|beam}\n",
    "    'beam_size':3\n",
    "}\n",
    "\n",
    "config_bert_resnet101 = {\n",
    "    ## BEGIN Bert Config - DO NOT CHANGE ##\n",
    "    'embedding.name':'bert-768',\n",
    "    'embedding.folder': 'bert-768',\n",
    "    'append_sos_eos_tokens': False,\n",
    "    'embedding.dim': 768,\n",
    "    'start_token': '[CLS]',\n",
    "    'end_token': '[SEP]',\n",
    "    'unknown_token': '[UNK]',\n",
    "    'pad_token': '[PAD]',\n",
    "    ## END Bert Config ##\n",
    "\n",
    "    'cnn': 'resnet101', # {resnet101,vgg16,resnext101}\n",
    "    'encoder_dim': 2048,\n",
    "    'encoder': 'lstm', #{lstm|transformer}\n",
    "    'search' : 'greedy',#{greedy|beam}\n",
    "    'beam_size':3\n",
    "}\n",
    "\n",
    "config_bert_filtered_resnet101_transformer = {\n",
    "    ## BEGIN Bert Config - DO NOT CHANGE ##\n",
    "    'embedding.name':'bert-768-word-freq-2-plus',\n",
    "    'embedding.folder': 'bert-768-word-freq-2-plus',\n",
    "    'append_sos_eos_tokens': False,\n",
    "    'embedding.dim': 768,\n",
    "    'start_token': '[CLS]',\n",
    "    'end_token': '[SEP]',\n",
    "    'unknown_token': '[UNK]',\n",
    "    'pad_token': '[PAD]',\n",
    "    ## END Bert Config ##\n",
    "\n",
    "    'cnn': 'resnet101', # {resnet101,vgg16,resnext101}\n",
    "    'encoder_dim': 2048,\n",
    "    'encoder': 'transformer', #{lstm|transformer}\n",
    "    'search' : 'greedy',#{greedy|beam}\n",
    "    'beam_size':3\n",
    "}\n",
    "\n",
    "config_bert_resnext101_transformer = {\n",
    "    ## BEGIN Bert Config - DO NOT CHANGE ##\n",
    "    'embedding.name':'bert-768',\n",
    "    'embedding.folder': 'bert-768',\n",
    "    'append_sos_eos_tokens': False,\n",
    "    'embedding.dim': 768,\n",
    "    'start_token': '[CLS]',\n",
    "    'end_token': '[SEP]',\n",
    "    'unknown_token': '[UNK]',\n",
    "    'pad_token': '[PAD]',\n",
    "    ## END Bert Config ##\n",
    "\n",
    "    'cnn': 'resnext101', # {resnet101,vgg16,resnext101}\n",
    "    'encoder_dim': 2048,\n",
    "    'encoder': 'transformer', #{lstm|transformer}\n",
    "    'search' : 'greedy',#{greedy|beam}\n",
    "    'beam_size':3\n",
    "}\n",
    "\n",
    "config_bert_resnext101 = {\n",
    "    ## BEGIN Bert Config - DO NOT CHANGE ##\n",
    "    'embedding.name':'bert-768',\n",
    "    'embedding.folder': 'bert-768',\n",
    "    'append_sos_eos_tokens': False,\n",
    "    'embedding.dim': 768,\n",
    "    'start_token': '[CLS]',\n",
    "    'end_token': '[SEP]',\n",
    "    'unknown_token': '[UNK]',\n",
    "    'pad_token': '[PAD]',\n",
    "    ## END Bert Config ##\n",
    "\n",
    "    'cnn': 'resnext101', # {resnet101,vgg16,resnext101}\n",
    "    'encoder_dim': 2048,\n",
    "    'encoder': 'lstm', #{lstm|transformer}\n",
    "    'search' : 'greedy',#{greedy|beam}\n",
    "    'beam_size':3\n",
    "}\n",
    "\n",
    "config_bert1024_resnet101 = {\n",
    "    ## BEGIN Bert Config - DO NOT CHANGE ##\n",
    "    'embedding.name':'bert-1024',\n",
    "    'embedding.folder': 'bert-1024',\n",
    "    'append_sos_eos_tokens': False,\n",
    "    'embedding.dim': 1024,\n",
    "    'start_token': '[CLS]',\n",
    "    'end_token': '[SEP]',\n",
    "    'unknown_token': '[UNK]',\n",
    "    'pad_token': '[PAD]',\n",
    "    ## END Bert Config ##\n",
    "\n",
    "    'cnn': 'resnet101', # {resnet101,vgg16,resnext101}\n",
    "    'encoder_dim': 2048,\n",
    "    'encoder': 'lstm', #{lstm|transformer}\n",
    "    'search' : 'greedy',#{greedy|beam}\n",
    "    'beam_size':3\n",
    "}\n",
    "\n",
    "config_bert1024_filtered_resnet101 = {\n",
    "    ## BEGIN Bert Config - DO NOT CHANGE ##\n",
    "    'embedding.name':'bert-1024-word-freq-2-plus',\n",
    "    'embedding.folder': 'bert-1024-word-freq-2-plus',\n",
    "    'append_sos_eos_tokens': False,\n",
    "    'embedding.dim': 1024,\n",
    "    'start_token': '[CLS]',\n",
    "    'end_token': '[SEP]',\n",
    "    'unknown_token': '[UNK]',\n",
    "    'pad_token': '[PAD]',\n",
    "    ## END Bert Config ##\n",
    "\n",
    "    'cnn': 'resnet101', # {resnet101,vgg16,resnext101}\n",
    "    'encoder_dim': 2048,\n",
    "    'encoder': 'lstm', #{lstm|transformer}\n",
    "    'search' : 'greedy',#{greedy|beam}\n",
    "    'beam_size':3\n",
    "}\n",
    "\n",
    "\n",
    "# conf = config_baseline\n",
    "#conf = config_bert_resnet101\n",
    "#conf = config_bert_filtered_resnet101_transformer\n",
    "#conf = config_bert_resnext101_transformer\n",
    "#conf = config_bert_resnext101\n",
    "# conf = config_bert1024_resnet101\n",
    "conf = config_bert1024_filtered_resnet101\n",
    "\n",
    "conf['dataset_size'] = 'small' # {small|medium|full}\n",
    "\n",
    "START_TOKEN = conf['start_token']\n",
    "END_TOKEN = conf['end_token']\n",
    "UNKNOWN_TOKEN = conf['unknown_token']\n",
    "PAD_TOKEN = conf['pad_token']\n",
    "\n",
    "print(conf)\n",
    "print(START_TOKEN)\n",
    "print(END_TOKEN)\n",
    "print(UNKNOWN_TOKEN)\n",
    "print(PAD_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOiksFtoeHKQ"
   },
   "source": [
    "## Create Experiment Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AM--9rPFeGEw"
   },
   "outputs": [],
   "source": [
    "% cd \"/content/drive/MyDrive/cs7643/project/\"\n",
    "import os, datetime\n",
    "start_time = datetime.datetime.now()\n",
    "exp_dir = os.path.join(os.getcwd(),  \"Experiment_\" + datetime.datetime.now().strftime('%Y%m%d_%H%M%S') + \"_\" + conf['embedding.name'] + \"_\" + conf['cnn'] + \"_\" + conf['encoder'] + \"_\" + conf['search'])\n",
    "os.makedirs(exp_dir)\n",
    "% cd $exp_dir\n",
    "% cp -r ../ExperimentTemplate/* .\n",
    "% ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSCcIRS1fECU"
   },
   "source": [
    "## Image Databunch  Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfYIl_OCfECU"
   },
   "source": [
    "### Load Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y76hFFiffECU"
   },
   "outputs": [],
   "source": [
    "# load caption data\n",
    "captions_file = 'captions/{}/captions.json'.format(conf['embedding.folder'])\n",
    "print(captions_file)\n",
    "with open(captions_file,'r') as f:\n",
    "    captions_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RFdwzqpTfECV"
   },
   "outputs": [],
   "source": [
    "# reformat Json to pd Dataframe\n",
    "image_meta = pd.DataFrame(captions_data['images'])\n",
    "image_meta['references'] = image_meta['sentences'].apply(lambda x: [i['tokens'] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvcQ3b_OfECV"
   },
   "outputs": [],
   "source": [
    "captions_data['images'][0]['sentences'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLASV9_QfECW"
   },
   "outputs": [],
   "source": [
    "image_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZotaYMt44YA"
   },
   "source": [
    "### Load Embedding and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7K7GbhP0fECY"
   },
   "outputs": [],
   "source": [
    "vocab_file = 'captions/{}/vocab.json'.format(conf['embedding.folder'])\n",
    "print(vocab_file)\n",
    "with open(vocab_file,'rb') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "def unnesting(df, explode_cols):\n",
    "    idx = df.index.repeat(df[explode_cols[0]].str.len())\n",
    "    df1 = pd.concat([\n",
    "        pd.DataFrame({x: np.concatenate(df[x].values)}) for x in explode_cols], axis=1)\n",
    "    df1.index = idx\n",
    "\n",
    "    return df1.join(df.drop(explode_cols, 1), how='left')\n",
    "\n",
    "# expand multi captions into rows (explode)\n",
    "metadata = unnesting(image_meta,['sentids','sentences'])\n",
    "metadata['labels'] =  metadata.sentences.apply(lambda x: x['raw'])\n",
    "metadata['tokens'] =  metadata.sentences.apply(lambda x: x['tokens'])\n",
    "metadata.reset_index(inplace = True)\n",
    "\n",
    "\n",
    "# attaching Image folder path to filename and add column with filepaths\n",
    "metadata['filename'] = metadata.filename.apply(lambda x: f'Flicker8k/Flickr8k_Dataset/Flicker8k_Dataset/{x}')\n",
    "metadata.reset_index(inplace = True)\n",
    "\n",
    "# numericalize tokens and re format list to numeric array\n",
    "if conf['append_sos_eos_tokens'] == True:\n",
    "  metadata['numericalized'] = metadata.tokens.apply(lambda x: [vocab[START_TOKEN]] + [vocab[i] if i in vocab.keys() else vocab[UNKNOWN_TOKEN] for i in x]+[vocab[END_TOKEN]])\n",
    "else:\n",
    "  metadata['numericalized'] = metadata.tokens.apply(lambda x: [vocab[i] if i in vocab.keys() else vocab[UNKNOWN_TOKEN] for i in x])\n",
    "\n",
    "metadata['numericalized'] = metadata.numericalized.apply(lambda x: np.array(x))\n",
    "metadata['SeqLen'] = metadata.numericalized.apply(len)\n",
    "\n",
    "# numerricalise references \n",
    "def ref_numericalize(lst): return list(map(lambda x: [vocab[i] if i in vocab.keys() else vocab[UNKNOWN_TOKEN] for i in x],lst))\n",
    "\n",
    "# store corresaponding reference captions in numeric form\n",
    "metadata['numericalized_ref'] = metadata.references.apply(ref_numericalize)\n",
    "\n",
    "# shuffle \n",
    "metadata = shuffle(metadata)\n",
    "metadata.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZcDiNU8fECZ"
   },
   "outputs": [],
   "source": [
    "metadata.numericalized_ref[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIVHcZwNJnct"
   },
   "source": [
    "### Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eSD9aIXfECZ"
   },
   "outputs": [],
   "source": [
    "# split data in train and valid\n",
    "train_idx = metadata.index[metadata.split == 'train']\n",
    "valid_idx = metadata.index[metadata.split == 'val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pUjf6VUfECZ"
   },
   "outputs": [],
   "source": [
    "if conf['dataset_size'] == 'small':\n",
    "  train_idx = train_idx[:10].to_list()\n",
    "  valid_idx = valid_idx[:2].to_list()\n",
    "  metadata = metadata[metadata.index.isin(train_idx+valid_idx)]\n",
    "  metadata.reset_index(drop=True, inplace=True)\n",
    "elif conf['dataset_size'] == 'medium':\n",
    "  train_idx = train_idx[:1000].to_list()\n",
    "  valid_idx = valid_idx[:200].to_list()\n",
    "  metadata = metadata[metadata.index.isin(train_idx+valid_idx)]\n",
    "  metadata.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1h6c_TDCfECa"
   },
   "source": [
    "### Custom Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBzWc8IHfECa"
   },
   "outputs": [],
   "source": [
    "# define Dataset object that outputs img path, caption, indices of reference captions \n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, data, split = 'train',transform=None):\n",
    "        data = data[data.split == split]\n",
    "        self.filenames = list(data['filename'])\n",
    "        self.captions  = list(data['numericalized'])\n",
    "        self.inds  = data.index.tolist()\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Grayscale images in dataset have to be onverted as tensor shapes have to match except in dim=0\n",
    "        image = Image.open(self.filenames[idx]).convert('RGB')\n",
    "        caption = self.captions[idx]\n",
    "        ref_ind = self.inds[idx]\n",
    "\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return (image, caption, ref_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkvUmcjDfECb"
   },
   "outputs": [],
   "source": [
    "# transformation resize img size to 350 px, tensoring, then normalize using image net stats\n",
    "trans = transforms.Compose([\n",
    "    transforms.Resize((350,350)),\n",
    "    transforms.ToTensor()\n",
    "    ,transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.5238/0.3159, -0.5003/0.3091, -0.4718/0.3216],\n",
    "    std=[1/0.3159, 1/0.3091, 1/0.3216]\n",
    ")\n",
    "\n",
    "denorm = transforms.Compose([\n",
    "    inv_normalize,\n",
    "    transforms.functional.to_pil_image\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5wFKgHbfECb"
   },
   "outputs": [],
   "source": [
    "# create Dataset instense for both train set and valid\n",
    "train_dataset = ImageCaptionDataset(metadata,'train',trans)\n",
    "valid_dataset = ImageCaptionDataset(metadata,'val',trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YcDJnUMFzU6p"
   },
   "outputs": [],
   "source": [
    "# check random sample\n",
    "#idx = 500\n",
    "#d = train_dataset\n",
    "#img, cap,ref = d[idx]\n",
    "\n",
    "#print(' '.join([dict(zip(vocab.values(),vocab.keys()))[x] for x in cap])+'\\n')\n",
    "\n",
    "\n",
    "#for cap in metadata.numericalized_ref.loc[ref]:\n",
    "    #print(' '.join([dict(zip(vocab.values(),vocab.keys()))[i] for i in cap]))\n",
    "#print(cap)\n",
    "#plt.imshow(denorm(img));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMghhyIufECe"
   },
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzZ2TPUh9C3A"
   },
   "source": [
    "### Baseline Encoder/Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMMPLQdcfECe"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F, init\n",
    "from torchvision import transforms, models\n",
    "import torch\n",
    "import random\n",
    "from pdb import set_trace\n",
    "\n",
    "\n",
    "device =torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# create a embedding layer\n",
    "def create_emb(embedding_array):\n",
    "    emb = nn.Embedding(len(word_map),embedding_dim)\n",
    "    emb.weight.data = torch.from_numpy(embedding_array).float()\n",
    "    return emb\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,encode_img_size, fine_tune = False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_imgsize = encode_img_size\n",
    "        if conf['cnn'] == 'resnet101':\n",
    "          cnn = models.resnet101(pretrained=True)\n",
    "        elif conf['cnn'] == 'vgg16':\n",
    "          cnn = models.vgg16(pretrained=True)\n",
    "        elif conf['cnn'] == 'resnext101':\n",
    "          cnn =  models.resnext101_32x8d(pretrained=True)\n",
    "\n",
    "        self.encoder = nn.Sequential(*list(cnn.children())[:-2]) # removing final Linear layer\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encode_img_size,encode_img_size))\n",
    "        self.fine_tune = fine_tune\n",
    "        self.fine_tune_h()\n",
    "        \n",
    "    def fine_tune_h(self):\n",
    "        \"\"\"\n",
    "        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
    "        :param fine_tune: Allow?\n",
    "        \"\"\"\n",
    "        for p in self.encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
    "        for c in list(self.encoder.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = self.fine_tune\n",
    "        \n",
    "    def forward(self,X):\n",
    "        out = self.encoder(X) # X is tensor of size (batch size, 3 (RGB), input height, width)\n",
    "        out = self.adaptive_pool(out) # output (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        out = out.view(out.size(0), -1, out.size(3))\n",
    "        return out\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5, pretrained_embedding = None,teacher_forcing_ratio = 0):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim) \n",
    "        self.embedding = nn.Embedding(vocab_size,embed_dim)\n",
    "        self.lstm = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True) #use \n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # gate\n",
    "        self.pretrained_embedding = pretrained_embedding\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initilizes some parametes with values from the uniform Dist\n",
    "\n",
    "        \"\"\"\n",
    "        self.embedding.weight.data.uniform_(0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1,0.1)\n",
    "\n",
    "        # Kaiming initialization\n",
    "        #init.kaiming_normal_(self.init_h.weight, mode='fan_in')\n",
    "        #init.kaiming_normal_(self.init_c.weight, mode='fan_in')\n",
    "        #init.kaiming_normal_(self.f_beta.weight, mode='fan_in')\n",
    "        #init.kaiming_normal_(self.fc.weight, mode='fan_in')\n",
    "\n",
    "    def pretrained(self):\n",
    "        if self.pretrained_embedding is not None:\n",
    "            self.embedding.weight.data = torch.from_numpy(self.pretrained_embedding)\n",
    "            \n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        \n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "            \n",
    "    def forward(self,encoder_out, encoded_captions,decode_lengths,inds):\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "        num_pixels = encoder_out.size(1)\n",
    "        #embeddings = self.embedding(encoded_captions)\n",
    "        \n",
    "        ## initililize hidden encoding\n",
    "        h, c = self.init_hidden_state(encoder_out)\n",
    "        \n",
    "        #dec_out = torch.zeros(1,batch_size,self.decoder_dim).to(device) #uncomment for teacher forcing\n",
    "\n",
    "        decode_lengths = decode_lengths - 1\n",
    "        \n",
    "        max_len = max(decode_lengths).item()\n",
    "        \n",
    "        \n",
    "        # Create tensors to hold word predicion scores and alphas\n",
    "        predictions = torch.zeros(batch_size, max_len, vocab_size)\n",
    "        alphas = torch.zeros(batch_size, max_len, num_pixels)\n",
    "        \n",
    "        for t in range(max_len):\n",
    "            batch_size_t = sum([l.item() > t for l in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
    "                                                                h[:batch_size_t])\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            \n",
    "            # teacher forcing \n",
    "            use_teacher_forcing = True if random.random() < self.teacher_forcing_ratio else False\n",
    "            \n",
    "            \n",
    "            inp_emb = self.embedding(encoded_captions[:batch_size_t,t]).float() if  (use_teacher_forcing or t==0) else self.embedding(prev_word[:batch_size_t]).float()\n",
    "            #self.emb2dec_dim((embeddings[:batch_size_t, t, :]).float()) use syntax for teacher forcing\n",
    "            #inp_emb = inp_emb if (use_teacher_forcing or t==0) else dec_out.squeeze(0)[:batch_size_t] #uncomment to add teacher forcing\n",
    "            \n",
    "            h, c = self.lstm(\n",
    "                torch.cat([inp_emb, attention_weighted_encoding], dim=1),\n",
    "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:batch_size_t,t , :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "\n",
    "            _,prev_word = preds.max(dim=-1)\n",
    "        return predictions,decode_lengths, alphas, inds\n",
    "        \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.enc_att = nn.Linear(encoder_dim,attention_dim)\n",
    "        self.dec_att = nn.Linear(decoder_dim,attention_dim)\n",
    "        self.att = nn.Linear(attention_dim,1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # kaiming\n",
    "        #init.kaiming_normal_(self.enc_att.weight, mode='fan_in')\n",
    "        #init.kaiming_normal_(self.dec_att.weight, mode='fan_in')\n",
    "        #init.kaiming_normal_(self.att.weight, mode='fan_in')\n",
    "\n",
    "    def forward(self,encoder_out, decoder_hidden):\n",
    "        encoder_att = self.enc_att(encoder_out)\n",
    "        decoder_att = self.dec_att(decoder_hidden)\n",
    "        att = self.att(self.relu(encoder_att + decoder_att.unsqueeze(1))).squeeze(2) #testing added batchnorm \n",
    "        alpha = self.softmax(att)\n",
    "        attention_weighted_encoding = (encoder_out*alpha.unsqueeze(2)).sum(dim=1)\n",
    "        \n",
    "        return attention_weighted_encoding, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyl8BOAt9J19"
   },
   "source": [
    "### Transformer Encoder/Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOTsvCFB8gDf"
   },
   "outputs": [],
   "source": [
    "### Transformer Encoder/Decoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        print (f'Embedder init {vocab_size, d_model}')\n",
    "        self.d_model = d_model\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        #self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.embed = nn.Linear(vocab_size,d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print (f'Embedder forward {x.shape}')\n",
    "        return self.relu(self.embed(x)) \n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 200, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        pe = Variable(self.pe[:,:seq_len], requires_grad=False)\n",
    "        if x.is_cuda:\n",
    "            pe.cuda()\n",
    "        \n",
    "        #print (x.shape)\n",
    "        #print (pe.shape)\n",
    "        x = x + pe\n",
    "        d = self.dropout(x)\n",
    "        #print (f' PositionalEncoder x forward shape {d.shape}')\n",
    "        return d\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        \n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        \n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm\n",
    "\n",
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    \n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "        \n",
    "    output = torch.matmul(scores, v)\n",
    "    return output\n",
    "\n",
    "class MultiHeadAttentionT(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout).to(self.device)\n",
    "        self.out = nn.Linear(d_model, d_model).to(self.device)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into N heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * N * sl * d_model\n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "\n",
    "        # calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(bs, -1, self.d_model)\n",
    "        output = self.out(concat)\n",
    "        #print (f' MultiHeadAttentionT output forward shape {output.shape}')\n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
    "        super().__init__() \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        #print (f' FeedForward forward shape {x.shape}')\n",
    "        return x\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "\n",
    "        self.attn = MultiHeadAttentionT(heads, d_model, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, dropout=dropout)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        #print (f' EncoderLayer forward x shape {x.shape}')\n",
    "        return x\n",
    "    \n",
    "# build a decoder layer with two multi-head attention layers and\n",
    "# one feed-forward layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attn_1 = MultiHeadAttentionT(heads, d_model, dropout=dropout)\n",
    "        self.attn_2 = MultiHeadAttentionT(heads, d_model, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, \\\n",
    "        src_mask))\n",
    "        x2 = self.norm_3(x)\n",
    "        x = x + self.dropout_3(self.ff(x2))\n",
    "        print (f' DecoderLayer forward x shape {x.shape}')\n",
    "        return x\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torchvision.models\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encode_img_size=14, d_model=2048, N=6, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.enc_imgsize = encode_img_size\n",
    "\n",
    "        if conf['cnn'] == 'resnet101':\n",
    "          cnn = torchvision.models.resnet101(pretrained=True)\n",
    "        elif conf['cnn'] == 'vgg16':\n",
    "          cnn = torchvision.models.vgg16(pretrained=True)\n",
    "        elif conf['cnn'] == 'resnext101':\n",
    "          cnn =  torchvision.models.resnext101_32x8d(pretrained=True)\n",
    "        \n",
    "        self.encoder = nn.Sequential(*list(cnn.children())[:-2]) # removing final Linear layer\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encode_img_size,encode_img_size))\n",
    "        #self.embed = Embedder(encode_img_size, d_model)\n",
    "        self.embed = Embedder(d_model, d_model) # exp\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        #self.pe = PositionalEncoder(encode_img_size, dropout=dropout) # exp\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads, dropout=dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "        \n",
    "    def forward(self, src, mask=None):\n",
    "        out = self.encoder(src) # X is tensor of size (batch size, 3 (RGB), input height, width)\n",
    "        out = self.adaptive_pool(out) # output (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        out = out.view(out.size(0), -1, out.size(3))\n",
    "        #print (f'TEncoder forward out shape {out.shape}')\n",
    "        x = self.embed(out)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, mask)\n",
    "        x  = self.norm(x)\n",
    "        #print (f' TEncoder forward x shape {x.shape}')\n",
    "        return self.norm(x)\n",
    "    \n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "\n",
    "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, img, tokens, d_model=512, N=6, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = TEncoder(img, d_model, N, heads, dropout)\n",
    "        self.decoder = Decoder(tokens, d_model, N, heads, dropout)\n",
    "        self.out = nn.Linear(d_model, tokens)\n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        e_outputs = self.encoder(src, src_mask)\n",
    "        #print(\"DECODER\")\n",
    "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-TEOVKUfECi"
   },
   "source": [
    "## DataLoaders  preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88yx7NILfECk"
   },
   "source": [
    "### Custom collate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wf1T68H-fECk"
   },
   "source": [
    "create a collate funtion with inputs bacth of imgs, caption pairs and sorts them by caption length in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlHwrMIBfECl"
   },
   "outputs": [],
   "source": [
    "\n",
    "from functools import partial\n",
    "def pad_collate_ImgCap(samples, pad_idx = 0, pad_first:bool=True, backwards:bool=False, transpose:bool=False, device = device):\n",
    "    \"Function that collect samples and adds padding. Flips token order if needed\"\n",
    "    images, captions, ref_ind = zip(*samples)\n",
    "    max_len_cap = max([len(c) for c in captions])\n",
    "    decode_lengths = torch.tensor([len(c) for c in captions])\n",
    "    res_cap = torch.zeros(len(samples), max_len_cap).long() + pad_idx\n",
    "    ref_ind = torch.tensor(ref_ind)\n",
    "    \n",
    "    if backwards: pad_first = not pad_first\n",
    "    for i,c in enumerate(captions):\n",
    "        if pad_first: \n",
    "            res_cap[i,-len(c):] = LongTensor(c)\n",
    "        else:         \n",
    "            res_cap[i,:len(c)] = LongTensor(c)\n",
    "    \n",
    "    if backwards:\n",
    "        cap = cap.flip(1)\n",
    "    if transpose:\n",
    "        res_cap.transpose_(0,1)\n",
    "    \n",
    "    images = torch.stack(images, 0, out=None)\n",
    "    \n",
    "    # Sort input data by decreasing lengths; why? apparent below\n",
    "    decode_lengths, sort_ind = decode_lengths.sort(dim=0, descending=True)\n",
    "    #set_trace()\n",
    "    \n",
    "    images = images[sort_ind].to(device)\n",
    "    res_cap = res_cap[sort_ind].to(device)\n",
    "    ref_ind = ref_ind[sort_ind].to(device)\n",
    "    decode_lengths = decode_lengths.to(device)\n",
    "    \n",
    "    \n",
    "    return (images, res_cap, decode_lengths,ref_ind), res_cap[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Jv_EaK0fECm"
   },
   "outputs": [],
   "source": [
    "imgcap_collate_func = partial(pad_collate_ImgCap, pad_idx=0, pad_first=False, transpose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECFggqUVfECm"
   },
   "outputs": [],
   "source": [
    "data_loader_batch_size=25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBgJ7xw0fECm"
   },
   "source": [
    "### Create databunch with transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gJgIMbIfECm"
   },
   "outputs": [],
   "source": [
    "### train and validation dataloaders\n",
    "train_sam = list(metadata.loc[metadata.split == 'train','numericalized'])\n",
    "val_sam = list(metadata.loc[metadata.split == 'val','numericalized'])\n",
    "\n",
    "### define Sampler for smapling bacthes for sorted pairs \n",
    "val_sampler = SortSampler(val_sam, key=lambda x: len(val_sam[x]))\n",
    "trn_sampler = SortishSampler(train_sam, key=lambda x: len(train_sam[x]), bs=data_loader_batch_size)\n",
    "\n",
    "### define data loaders for loading inputs batches into network\n",
    "val_dl = DataLoader(dataset=valid_dataset, batch_size=data_loader_batch_size, sampler=val_sampler, collate_fn=imgcap_collate_func,pin_memory=False)\n",
    "trn_dl = DataLoader(dataset=train_dataset, batch_size=data_loader_batch_size, sampler=trn_sampler, collate_fn=imgcap_collate_func,pin_memory=False)\n",
    "\n",
    "\n",
    "# transformations \n",
    "tfms = get_transforms(flip_vert=False, max_lighting=0.1, max_zoom=1.05, max_warp=0.)\n",
    "\n",
    "# fastai databunch object \n",
    "dataBunch = DataBunch(train_dl=trn_dl, valid_dl=val_dl ,device=device,collate_fn=imgcap_collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yw1aWB2NfECn"
   },
   "outputs": [],
   "source": [
    "dataBunch.valid_ds[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1V7c_bMfECn"
   },
   "outputs": [],
   "source": [
    "# visualize sample \n",
    "##idx = 10000\n",
    "##d = dataBunch.valid_ds\n",
    "#img, cap,_ = d[-1]\n",
    "\n",
    "#print(' '.join([dict(zip(vocab.values(),vocab.keys()))[x] for x in cap]))\n",
    "#print(cap)\n",
    "#plt.imshow(denorm(img));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZtYkmChfECo"
   },
   "source": [
    "## Model Intialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H73Ui7VJfECo"
   },
   "source": [
    "### HyperParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftJzlSrgKPtC"
   },
   "source": [
    "Tune HyperParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ra0UEHppKTNP"
   },
   "outputs": [],
   "source": [
    "emb_dim = conf['embedding.dim']\n",
    "attention_dim = 512 # encoder_dim tranformed to attention_dim\n",
    "decoder_dim = 512  #  word_emb_dim tranformed to decoder_dim\n",
    "dropout = 0.5\n",
    "encoder_dim = conf['encoder_dim']\n",
    "vocab_size = len(vocab)\n",
    "fine_tune_encoder = True\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2rWwywRfECo"
   },
   "source": [
    "Load Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbB0J4defECo"
   },
   "outputs": [],
   "source": [
    "# load word embeddings\n",
    "embedding_file = 'captions/{}/embedding.pkl'.format(conf['embedding.folder'])\n",
    "print(embedding_file)\n",
    "\n",
    "with open(embedding_file,'rb') as f:\n",
    "    embedding = pickle.load(f)\n",
    "    if conf['embedding.name'] == 'fasttext-word-freq-2-plus':\n",
    "      embedding[4021:] = np.random.normal(embedding.mean(),embedding.std(),(4, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFerN06Amsxr"
   },
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrIilKqcfECp"
   },
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "#import numpy as np\n",
    "\n",
    "\n",
    "#x = range(embedding.shape[0])\n",
    "#y = range(embedding.shape[1])\n",
    "\n",
    "\n",
    "#hf = plt.figure(figsize=(15,20))\n",
    "#ha = hf.add_subplot(111, projection='3d')\n",
    "\n",
    "#X, Y = np.meshgrid(x, y)\n",
    "\n",
    "##ha.scatter(X, Y, embedding)\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZobvO7EQMHT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcze1CYgfECp"
   },
   "source": [
    "### Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qunYFMHVfECq"
   },
   "outputs": [],
   "source": [
    "###########   Layer Initializations ##########\n",
    "\n",
    "encode_img_size = 14\n",
    "\n",
    "if conf['encoder'] == 'transformer':\n",
    "  enc = TransformerEncoder()\n",
    "else:\n",
    "  enc = Encoder(encode_img_size,fine_tune=fine_tune_encoder)\n",
    "\n",
    "dec = Decoder(attention_dim, \n",
    "              emb_dim, \n",
    "              decoder_dim, \n",
    "              vocab_size, \n",
    "              encoder_dim=encoder_dim, \n",
    "              dropout=dropout, \n",
    "              pretrained_embedding = embedding,teacher_forcing_ratio=1)\n",
    "\n",
    "\n",
    "# Models Ensemble \n",
    "class Ensemble(nn.Module):\n",
    "    def __init__(self,encoder, decoder):\n",
    "        super(Ensemble, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self,x1,caps,decode_lengths,inds): # you need flatten in between\n",
    "        imgs = self.encoder(x1) # here input x1 is Images: output (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
    "        scores,decode_lengths,alphas, inds  = self.decoder(imgs, caps,decode_lengths,inds) #caps_sorted, decode_lengths, alphas, sort_ind\n",
    "        return scores,decode_lengths, alphas, inds\n",
    "\n",
    "# Testing\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "enc = enc.to(device)\n",
    "dec = dec.to(device)\n",
    "arch = Ensemble(enc, dec).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpfCvqzhL9DS"
   },
   "outputs": [],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Q504pQAxNVe"
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f29Ikjp4AOE6"
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZW9xO_WfECq"
   },
   "outputs": [],
   "source": [
    "def eval(img):\n",
    "    encoder_out = self.model.encoder(img)\n",
    "\n",
    "    num_pixels = encoder_out.size(1)\n",
    "    encoder_out = encoder_out.expand(3, num_pixels, encoder_dim)\n",
    "\n",
    "\n",
    "    # Tensor to store top k previous words at each step; now they're just 0\n",
    "    k_prev_words = torch.LongTensor([[0]] * k).to(device)  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences; now they're just <start>\n",
    "    seqs = k_prev_words  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences' scores; now they're just 0\n",
    "    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "\n",
    "\n",
    "    #dec_inp = torch.zeros(h.size(1), requires_grad=False).long()\n",
    "    #dec_inp = dec_inp.to(self.device)\n",
    "\n",
    "    complete_seqs = list()\n",
    "    complete_seqs_scores = list()\n",
    "\n",
    "    step = 1\n",
    "\n",
    "    h,c = self.model.decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "    a = True\n",
    "    while True:\n",
    "        k_prev_words = torch.LongTensor([[0]] * 3).to(device)\n",
    "        embeddings = self.model.decoder.embedding(k_prev_words).squeeze(1)\n",
    "        awe, _ = self.model.decoder.attention(encoder_out, h)\n",
    "        gate = self.model.decoder.sigmoid(self.model.decoder.f_beta(h))\n",
    "        awe = gate * awe\n",
    "        h, c = self.model.decoder.lstm(torch.cat([embeddings, awe], dim=1), (h, c))\n",
    "\n",
    "        scores = self.model.decoder.fc(h)  # (s, vocab_size)\n",
    "        scores = F.log_softmax(scores, dim=1)\n",
    "        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "        scores = top_k_scores.expand_as(scores) + scores\n",
    "        if step == 1:\n",
    "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)\n",
    "        else:\n",
    "            # Unroll and find top scores, and their unrolled indices\n",
    "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "\n",
    "\n",
    "        # Convert unrolled indices to actual indices of scores\n",
    "        prev_word_inds = top_k_words / vocab_size  # (s)\n",
    "        next_word_inds = top_k_words % vocab_size  # (s)\n",
    "\n",
    "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)\n",
    "\n",
    "\n",
    "        # If any sequence is not complete\n",
    "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
    "                                       next_word != pad_idx]\n",
    "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "        # if at step any of seq completes we store complete_seqs and proceeds with\n",
    "        # remaining k - len(complete_seqs) incomplete seqs \n",
    "\n",
    "        if len(complete_inds) > 0:\n",
    "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "        k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "        if k == 0:\n",
    "            break\n",
    "\n",
    "        seqs = seqs[incomplete_inds]\n",
    "        h = h[prev_word_inds[incomplete_inds]]\n",
    "        c = c[prev_word_inds[incomplete_inds]]\n",
    "\n",
    "        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "        # Break if things have been going on too long\n",
    "        if step > 50:\n",
    "            break\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qp6KKWjDfECq"
   },
   "source": [
    "### Callback handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6fkQs0GfECq"
   },
   "source": [
    "Defining Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-afXPYbfECr"
   },
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "from fastai.callback import Callback\n",
    "import copy as cp\n",
    "from torch import nn\n",
    "from fastai.vision import *\n",
    "from pathlib import  Path, posixpath\n",
    "from pdb import set_trace\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "from statistics import mean\n",
    "from fastai.callback import Callback\n",
    "import copy as cp\n",
    "from torch import nn\n",
    "from fastai.vision import *\n",
    "from pathlib import  Path, posixpath\n",
    "from pdb import set_trace\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def beam_search(mod, img,vocab = None, beam_size = 3):\n",
    "    with torch.no_grad():\n",
    "        k = beam_size\n",
    "        \n",
    "        ## imput tensor preparation\n",
    "        img = img.unsqueeze(0) #treating as batch of size 1\n",
    "\n",
    "        ## model prepartion\n",
    "        #mod = learn.model\n",
    "\n",
    "        # encoder output\n",
    "        encoder_out = mod.encoder(img)\n",
    "        #encoder_dim = encoder_out.size(-1)\n",
    "        encoder_size = encoder_out.size(-1)\n",
    "        #num_pixels = encoder_out.size(1)\n",
    "        encoder_out = encoder_out.view(1, -1, encoder_size)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "        # expand or repeat 'k' time\n",
    "        #encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
    "        encoder_out = encoder_out.expand(k, num_pixels, encoder_size)  # (k, num_pixels, encoder_dim)\n",
    "        # Tensor to store top k previous words at each step; now they're just <start>\n",
    "        #k_prev_words = torch.LongTensor([[vocab['<start>']]] * k).to(device)  # (k, 1)\n",
    "        k_prev_words = torch.LongTensor([[1]] * k).to(device) \n",
    "        # Tensor to store top k sequences; now they're just <start>\n",
    "        seqs = k_prev_words       \n",
    "        # Tensor to store top k sequences' scores; now they're just 0\n",
    "        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "        # Lists to store completed sequences and scores\n",
    "        complete_seqs = list()\n",
    "        complete_seqs_scores = list()\n",
    "\n",
    "        # Start decoding\n",
    "        step = 1\n",
    "        h, c = mod.decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "        references = list()\n",
    "        hypotheses = list()\n",
    "\n",
    "        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "        while True:\n",
    "            embeddings = mod.decoder.embedding(k_prev_words).squeeze(1).float()  # (s, embed_dim)\n",
    "            awe, _ = mod.decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
    "            gate = mod.decoder.sigmoid(mod.decoder.f_beta(h))\n",
    "            awe = (gate * awe)\n",
    "\n",
    "            h, c = mod.decoder.lstm(torch.cat([embeddings, awe], dim=1), (h, c))\n",
    "            scores = mod.decoder.fc(h)\n",
    "            scores = F.log_softmax(scores, dim=1)\n",
    "            \n",
    "\n",
    "            # Add scores to prev scores\n",
    "            scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
    "\n",
    "            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "            if step == 1:\n",
    "                top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "            else:\n",
    "                # Unroll and find top scores, and their unrolled indices\n",
    "                top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "            # Convert unrolled indices to actual indices of scores\n",
    "            #prev_word_inds = torch.LongTensor(top_k_words // len(vocab)).to(device)  # (s)\n",
    "            prev_word_inds = torch.true_divide(top_k_words , len(vocab)).long().cpu()\n",
    "            next_word_inds = top_k_words % len(vocab)  # (s)\n",
    "\n",
    "            # Add new words to sequences\n",
    "            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1) stroes indices of words\n",
    "\n",
    "            # Which sequences are incomplete (didn't reach <end>)?\n",
    "            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
    "                                next_word != vocab[END_TOKEN]]\n",
    "\n",
    "            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "            # Set aside complete sequences\n",
    "            if len(complete_inds) > 0:\n",
    "                complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "                complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "            k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "            # Proceed with incomplete sequences\n",
    "            if k == 0:\n",
    "                break\n",
    "            seqs = seqs[incomplete_inds]\n",
    "            h = h[prev_word_inds[incomplete_inds]]\n",
    "            c = c[prev_word_inds[incomplete_inds]]\n",
    "            encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "\n",
    "            # Break if things have been going on too long\n",
    "            if step > 50:\n",
    "                break\n",
    "            step += 1\n",
    "\n",
    "    if len(complete_seqs_scores) > 0:\n",
    "      i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "      seq = complete_seqs[i]\n",
    "    else:\n",
    "       seq = [vocab[START_TOKEN],vocab[END_TOKEN]]\n",
    "    # Hypotheses\n",
    "    hypotheses.append([w for w in seq if w not in {vocab[START_TOKEN], vocab[END_TOKEN], vocab[PAD_TOKEN]}])\n",
    "\n",
    "    return hypotheses\n",
    "\n",
    "# Loss Function\n",
    "def loss_func(input,targets, lamb=1):\n",
    "    pred, decode_lengths, alphas,_ = input\n",
    "    pred = pack_padded_sequence(pred, decode_lengths.cpu(), batch_first=True).to(device)\n",
    "    targs = pack_padded_sequence(targets, decode_lengths.cpu(), batch_first=True).to(device)\n",
    "    loss = nn.CrossEntropyLoss().to(device)(pred.data, targs.data.long())\n",
    "    loss += (lamb*((1. - alphas.sum(dim=1)) ** 2.).mean()).to(device) #stochastic attention\n",
    "    return  loss #loss(pred.data.long(), targets.data.long())\n",
    "\n",
    "\n",
    "\n",
    "def topK_accuracy(input, targets, k=5):\n",
    "    \"\"\"\n",
    "    Computes top-k accuracy, from predicted and true labels.\n",
    "    :param scores: scores from the model\n",
    "    :param targets: true labels\n",
    "    :param k: k in top-k accuracy\n",
    "    :return: top-k accuracy\n",
    "    \"\"\"\n",
    "    pred, decode_lengths, alphas,_ = input\n",
    "    batch_size = targets.size(0)\n",
    "    scores = pack_padded_sequence(pred, decode_lengths.cpu(), batch_first=True).to(device)\n",
    "    targ = pack_padded_sequence(targets, decode_lengths.cpu(), batch_first=True).to(device)\n",
    "    batch_size = targ.data.size(0)\n",
    "    _, ind = scores.data.topk(k, 1, True, True)\n",
    "    correct = ind.eq(targ.data.view(-1, 1).expand_as(ind))\n",
    "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
    "    return correct_total * (100.0 / batch_size)\n",
    "\n",
    "\n",
    "class TeacherForcingCallback(Callback):\n",
    "    def __init__(self, learn:Learner):\n",
    "        super().__init__()\n",
    "        self.learn = learn\n",
    "    \n",
    "    def on_batch_begin(self, epoch,**kwargs):\n",
    "        self.learn.model.decoder.teacher_forcing_ratio = (10 - epoch) * 0.1 if epoch < 10 else 0\n",
    "        \n",
    "    def on_batch_end(self,**kwargs):\n",
    "        self.learn.model.decoder.teacher_forcing_ratio = 0.\n",
    "\n",
    "class GradientClipping(LearnerCallback):\n",
    "    \"Gradient clipping during training.\"\n",
    "    def __init__(self, learn:Learner, clip:float = 0.3):\n",
    "        super().__init__(learn)\n",
    "        self.clip = clip\n",
    "\n",
    "    def on_backward_end(self, **kwargs):\n",
    "        \"Clip the gradient before the optimizer step.\"\n",
    "        if self.clip: nn.utils.clip_grad_norm_(self.learn.model.parameters(), self.clip)\n",
    "\n",
    "        \n",
    "\n",
    "class Bleu1Metric(Callback):\n",
    "    def __init__(self,metadata = None, vocab = None):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.bleureferences = list()\n",
    "        self.bleucandidates = list()\n",
    "\n",
    "        \n",
    "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "        pred, decode_lengths,_,inds = last_output\n",
    "        references = self.metadata.numericalized_ref.loc[inds.tolist()]\n",
    "        _,pred_words = pred.max(dim=-1)\n",
    "        pred_words, decode_lengths,references = list(pred_words), decode_lengths, list(references)\n",
    "        hypotheses = list()\n",
    "        for i,cap in enumerate(pred_words): hypotheses.append([x for x in cap.tolist()[:decode_lengths[i]] if x not in {self.vocab[START_TOKEN], self.vocab[END_TOKEN], self.vocab[PAD_TOKEN]}])\n",
    "        #for i,cap in enumerate(pred_words): hypotheses.append([x for x in cap.tolist() if x not in {self.vocab['xxunk'], self.vocab['xxpad'], self.vocab['xxbos'], self.vocab['xxeos'],self.vocab['xxfld'],self.vocab['xxmaj'],self.vocab['xxup'],self.vocab['xxrep'],self.vocab['xxwrep']}])\n",
    "        self.bleureferences.extend(references)\n",
    "        self.bleucandidates.extend(hypotheses)\n",
    "\n",
    "        \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        assert len(self.bleureferences) == len(self.bleucandidates)\n",
    "        # print('\\n'+' '.join([list(self.vocab.keys())[i-1] for i in self.bleucandidates[0]])+' | '+' '.join([list(self.vocab.keys())[i-1] for i in self.bleureferences[0][0]]))\n",
    "        # print(' '.join([list(self.vocab.keys())[i-1] for i in self.bleucandidates[25]])+' | '+' '.join([list(self.vocab.keys())[i-1] for i in self.bleureferences[25][0]]))\n",
    "        # print(' '.join([list(self.vocab.keys())[i-1] for i in self.bleucandidates[99]])+' | '+' '.join([list(self.vocab.keys())[i-1] for i in self.bleureferences[99][0]])+'\\n')\n",
    "        blue1 = corpus_bleu(self.bleureferences, self.bleucandidates,weights =(1.0/1.0,0,0,0))\n",
    "        return add_metrics(last_metrics,blue1)\n",
    "\n",
    "\n",
    "class Bleu2Metric(Callback):\n",
    "    def __init__(self,metadata = None, vocab = None):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.bleureferences = list()\n",
    "        self.bleucandidates = list()\n",
    "\n",
    "        \n",
    "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "        pred, decode_lengths,_,inds = last_output\n",
    "        references = self.metadata.numericalized_ref.loc[inds.tolist()]\n",
    "        _,pred_words = pred.max(dim=-1)\n",
    "        pred_words, decode_lengths,references = list(pred_words), decode_lengths, list(references)\n",
    "        hypotheses = list()\n",
    "        for i,cap in enumerate(pred_words): hypotheses.append([x for x in cap.tolist()[:decode_lengths[i]] if x not in {self.vocab[START_TOKEN], self.vocab[END_TOKEN], self.vocab[PAD_TOKEN]}])\n",
    "        #for i,cap in enumerate(pred_words): hypotheses.append([x for x in cap.tolist() if x not in {self.vocab['xxunk'], self.vocab['xxpad'], self.vocab['xxbos'], self.vocab['xxeos'],self.vocab['xxfld'],self.vocab['xxmaj'],self.vocab['xxup'],self.vocab['xxrep'],self.vocab['xxwrep']}])\n",
    "        self.bleureferences.extend(references)\n",
    "        self.bleucandidates.extend(hypotheses)\n",
    "\n",
    "        \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        assert len(self.bleureferences) == len(self.bleucandidates)\n",
    "        # print('\\n'+' '.join([list(self.vocab.keys())[i-1] for i in self.bleucandidates[0]])+' | '+' '.join([list(self.vocab.keys())[i-1] for i in self.bleureferences[0][0]]))\n",
    "        # print(' '.join([list(self.vocab.keys())[i-1] for i in self.bleucandidates[25]])+' | '+' '.join([list(self.vocab.keys())[i-1] for i in self.bleureferences[25][0]]))\n",
    "        # print(' '.join([list(self.vocab.keys())[i-1] for i in self.bleucandidates[99]])+' | '+' '.join([list(self.vocab.keys())[i-1] for i in self.bleureferences[99][0]])+'\\n')\n",
    "        blue2 = corpus_bleu(self.bleureferences, self.bleucandidates,weights =(1.0/2.0,1.0/2.0,0,0))\n",
    "        return add_metrics(last_metrics,blue2)\n",
    "\n",
    "\n",
    "class Bleu3Metric(Callback):\n",
    "    def __init__(self,metadata = None, vocab = None):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.bleureferences = list()\n",
    "        self.bleucandidates = list()\n",
    "\n",
    "        \n",
    "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "        pred, decode_lengths,_,inds = last_output\n",
    "        references = self.metadata.numericalized_ref.loc[inds.tolist()]\n",
    "        _,pred_words = pred.max(dim=-1)\n",
    "        pred_words, decode_lengths,references = list(pred_words), decode_lengths, list(references)\n",
    "        hypotheses = list()\n",
    "        for i,cap in enumerate(pred_words): hypotheses.append([x for x in cap.tolist()[:decode_lengths[i]] if x not in {self.vocab[START_TOKEN], self.vocab[END_TOKEN], self.vocab[PAD_TOKEN]}])\n",
    "        #for i,cap in enumerate(pred_words): hypotheses.append([x for x in cap.tolist() if x not in {self.vocab['xxunk'], self.vocab['xxpad'], self.vocab['xxbos'], self.vocab['xxeos'],self.vocab['xxfld'],self.vocab['xxmaj'],self.vocab['xxup'],self.vocab['xxrep'],self.vocab['xxwrep']}])\n",
    "        self.bleureferences.extend(references)\n",
    "        self.bleucandidates.extend(hypotheses)\n",
    "\n",
    "        \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        assert len(self.bleureferences) == len(self.bleucandidates)\n",
    "        # print('\\n'+' '.join([list(self.vocab.keys())[i-1] for i in self.bleucandidates[0]])+' | '+' '.join([list(self.vocab.keys())[i-1] for i in self.bleureferences[0][0]]))\n",
    "        # print(' '.join([list(self.vocab.keys())[i-1] for i in self.bleucandidates[25]])+' | '+' '.join([list(self.vocab.keys())[i-1] for i in self.bleureferences[25][0]]))\n",
    "        # print(' '.join([list(self.vocab.keys())[i-1] for i in self.bleucandidates[99]])+' | '+' '.join([list(self.vocab.keys())[i-1] for i in self.bleureferences[99][0]])+'\\n')\n",
    "        blue3 = corpus_bleu(self.bleureferences, self.bleucandidates,weights =(1.0/3.0,1.0/3.0,1.0/3.0,0))\n",
    "        return add_metrics(last_metrics,blue3)\n",
    "\n",
    "\n",
    "class Bleu4Metric(Callback):\n",
    "    def __init__(self,metadata = None, vocab = None):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.bleureferences = list()\n",
    "        self.bleucandidates = list()\n",
    "\n",
    "        \n",
    "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "        pred, decode_lengths,_,inds = last_output\n",
    "        references = self.metadata.numericalized_ref.loc[inds.tolist()]\n",
    "        _,pred_words = pred.max(dim=-1)\n",
    "        pred_words, decode_lengths,references = list(pred_words), decode_lengths, list(references)\n",
    "        hypotheses = list()\n",
    "        for i,cap in enumerate(pred_words): hypotheses.append([x for x in cap.tolist()[:decode_lengths[i]] if x not in {self.vocab[START_TOKEN], self.vocab[END_TOKEN], self.vocab[PAD_TOKEN]}])\n",
    "        #for i,cap in enumerate(pred_words): hypotheses.append([x for x in cap.tolist() if x not in {self.vocab['xxunk'], self.vocab['xxpad'], self.vocab['xxbos'], self.vocab['xxeos'],self.vocab['xxfld'],self.vocab['xxmaj'],self.vocab['xxup'],self.vocab['xxrep'],self.vocab['xxwrep']}])\n",
    "        self.bleureferences.extend(references)\n",
    "        self.bleucandidates.extend(hypotheses)\n",
    "\n",
    "        \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        assert len(self.bleureferences) == len(self.bleucandidates)\n",
    "        # print('\\n'+' '.join([list(self.vocab.keys())[i-1] for i in self.bleucandidates[0]])+' | '+' '.join([list(self.vocab.keys())[i-1] for i in self.bleureferences[0][0]]))\n",
    "        # print(' '.join([list(self.vocab.keys())[i-1] for i in self.bleucandidates[25]])+' | '+' '.join([list(self.vocab.keys())[i-1] for i in self.bleureferences[25][0]]))\n",
    "        # print(' '.join([list(self.vocab.keys())[i-1] for i in self.bleucandidates[99]])+' | '+' '.join([list(self.vocab.keys())[i-1] for i in self.bleureferences[99][0]])+'\\n')\n",
    "        #blue3 = corpus_bleu(self.bleureferences, self.bleucandidates,weights =(1.0/3.0,1.0/3.0,1.0/3.0,0))\n",
    "        bleu4 = corpus_bleu(self.bleureferences, self.bleucandidates,weights =(0.25,0.25,0.25,0.25))\n",
    "        return add_metrics(last_metrics,bleu4)\n",
    "\n",
    "\n",
    "class BeamSearch(LearnerCallback):\n",
    "    def __init__(self,learn:Learner,metadata = None, vocab = None, beam_fn = beam_search):\n",
    "        super().__init__(learn)\n",
    "        self.beam_fn = beam_fn\n",
    "        self.vocab = vocab\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.beamreferences = list()\n",
    "        self.beamcandidates = list()\n",
    "\n",
    "    def on_batch_end(self,last_input, last_target, **kwargs):\n",
    "        model_copy = cp.deepcopy(self.learn.model)\n",
    "        imgs,_,_,inds = last_input\n",
    "        references = self.metadata.numericalized_ref.loc[inds.tolist()]\n",
    "        references = list(references)\n",
    "        hypotheses = list()\n",
    "        for img in imgs: hypotheses.append(self.beam_fn(model_copy,img,self.vocab)[0])\n",
    "        self.beamreferences.extend(references)\n",
    "        self.beamcandidates.extend(hypotheses)\n",
    "\n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        assert len(self.beamreferences) == len(self.beamcandidates)\n",
    "        #print(' '.join([list(self.vocab.keys())[i-1] for i in self.beamcandidates[8]])+' | '+' '.join([list(self.vocab.keys())[i-1] for i in self.beamreferences[8][0]]))\n",
    "        return add_metrics(last_metrics,corpus_bleu(self.beamreferences, self.beamcandidates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Df1J1u8fECr"
   },
   "source": [
    "### Learner create Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXZ86_tOyPhi"
   },
   "outputs": [],
   "source": [
    "loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZ575UT_fECv"
   },
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "\n",
    "\n",
    "# metrics functions\n",
    "loss_func = partial(loss_func, lamb = 1)\n",
    "#BleuMetric(metadata,vocab)\n",
    "beam_fn = partial(beam_search,beam_size = conf['beam_size'])\n",
    "\n",
    "learn = Learner(dataBunch,arch,loss_func= loss_func,opt_func=opt_fn,  metrics=[topK_accuracy,Bleu1Metric(metadata,vocab),Bleu2Metric(metadata,vocab),Bleu3Metric(metadata,vocab),Bleu4Metric(metadata,vocab)],callback_fns=[ShowGraph]) #,TeacherForcingTurnOff,TeacherForcingCallback\n",
    "\n",
    "# split model into encoder and decoder layer groups\n",
    "split_list = [learn.model.encoder,learn.model.decoder]\n",
    "learn.split(split_list)\n",
    "len(learn.layer_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5X5906px5o6"
   },
   "outputs": [],
   "source": [
    "opt_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HNPtviZfECw"
   },
   "outputs": [],
   "source": [
    "# def summary_trainable(learner):\n",
    "#     result = []\n",
    "#     total_params_element = 0\n",
    "#     def check_trainable(module):\n",
    "#         nonlocal total_params_element\n",
    "#         if len(list(module.children())) == 0:\n",
    "#             num_param = 0\n",
    "#             num_trainable_param = 0\n",
    "#             num_param_numel = 0\n",
    "#             for parameter in module.parameters():\n",
    "#                 num_param += 1\n",
    "#                 if parameter.requires_grad:\n",
    "#                     num_param_numel += parameter.numel()\n",
    "#                     total_params_element += parameter.numel()\n",
    "#                     num_trainable_param += 1\n",
    "\n",
    "#             result.append({'module': module, 'num_param': num_param , 'num_trainable_param' : num_trainable_param, 'num_param_numel': num_param_numel})\n",
    "#     learner.model.apply(check_trainable)\n",
    "\n",
    "#     print(\"{: <85} {: <17} {: <20} {: <40}\".format('Module Name', 'Total Parameters', 'Trainable Parameters', '# Elements in Trainable Parametrs'))\n",
    "#     for row in result:\n",
    "#         print(\"{: <85} {: <17} {: <20} {: <40,}\".format(row['module'].__str__(), row['num_param'], row['num_trainable_param'], row['num_param_numel']))\n",
    "#     print('Total number of parameters elements {:,}'.format(total_params_element))\n",
    "\n",
    "\n",
    "\n",
    "# # uncomment below to print summary of trainable layers \n",
    "# #learn.freeze()\n",
    "# summary_trainable(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfctomKVfECx"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hks25PXJfECy"
   },
   "source": [
    "### Stage 1: with encoder part freezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MddLYaKB3dW"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUt2VJ0BfECy"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# learn.freeze()\n",
    "# learn.lr_find(end_lr = 1)\n",
    "# learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_FXGup6fECy",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### train for 10 epochs\n",
    "if conf['search'] == 'beam':\n",
    "  learn.freeze()\n",
    "  EPOCH = 10\n",
    "  TUNED_LEARNING_RATE = 5e-4\n",
    "  learn.fit(EPOCH,TUNED_LEARNING_RATE,callbacks = [SaveModelCallback(learn, monitor='bleu4_metric',name='Stage_1_Best_Model'),\n",
    "                                          GradientClipping(learn = learn, clip=5.)\n",
    "                                          ,BeamSearch(learn = learn,metadata = metadata, vocab = vocab, beam_fn = beam_fn)])\n",
    "else:\n",
    "  learn.freeze()\n",
    "  EPOCH = 10\n",
    "  TUNED_LEARNING_RATE = 5e-4\n",
    "  learn.fit(EPOCH,TUNED_LEARNING_RATE,callbacks = [SaveModelCallback(learn, monitor='bleu4_metric',name='Stage_1_Best_Model'),\n",
    "                                          GradientClipping(learn = learn, clip=5.)])\n",
    "                                          #,BeamSearch(learn = learn,metadata = metadata, vocab = vocab, beam_fn = beam_fn)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tbmg0st8fECz"
   },
   "source": [
    "### Stage-2: Unfreeze encoder part as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QRaSEWCfECz"
   },
   "outputs": [],
   "source": [
    "# learn.data.batch_size = BATCH_SIZE\n",
    "# learn.unfreeze()\n",
    "# learn.load('Stage_1_Best_Model');\n",
    "# learn.lr_find(start_lr=1e-11,end_lr = 1e-03)\n",
    "# learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-5oFxHtfECz",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if conf['search'] == 'beam':\n",
    "  learn.data.batch_size = BATCH_SIZE\n",
    "  learn.unfreeze()\n",
    "  learn.load('Stage_1_Best_Model');\n",
    "  EPOCH = 10\n",
    "  TUNED_LEARNING_RATE = 1e-4\n",
    "  learn.fit_one_cycle(EPOCH, TUNED_LEARNING_RATE,\n",
    "                      callbacks = [SaveModelCallback(learn, monitor='bleu4_metric',name='Stage_2_Best_Model'),\n",
    "                                  GradientClipping(learn = learn, clip=5.)\n",
    "                                  ,BeamSearch(learn = learn,metadata = metadata, vocab = vocab, beam_fn = beam_fn)])\n",
    "else:\n",
    "  learn.data.batch_size = BATCH_SIZE\n",
    "  learn.unfreeze()\n",
    "  learn.load('Stage_1_Best_Model');\n",
    "  EPOCH = 10\n",
    "  TUNED_LEARNING_RATE = 1e-4\n",
    "  learn.fit_one_cycle(EPOCH, TUNED_LEARNING_RATE,\n",
    "                      callbacks = [SaveModelCallback(learn, monitor='bleu4_metric',name='Stage_2_Best_Model'),\n",
    "                                  GradientClipping(learn = learn, clip=5.)])\n",
    "                                  #,BeamSearch(learn = learn,metadata = metadata, vocab = vocab, beam_fn = beam_fn)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Kjpb6kfF82F"
   },
   "outputs": [],
   "source": [
    "end_time = datetime.datetime.now()\n",
    "print(\"total time={}\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1Np8n42fEC0"
   },
   "outputs": [],
   "source": [
    "# learn.save('last_epoch_stage2');"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Initialization Cell",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Project_Final_v5",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "868.889px",
    "left": "1653.99px",
    "top": "641.465px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
